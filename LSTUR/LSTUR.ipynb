{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "historic-worry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) \n",
      "[GCC 9.3.0]\n",
      "Tensorflow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "#import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
    "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
    "from recommenders.models.newsrec.models.lstur import LSTURModel\n",
    "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-headset",
   "metadata": {},
   "source": [
    "# LSTUR: Neural News Recommendation with Long- and Short-term User Representations\n",
    "\n",
    "https://github.com/microsoft/recommenders/blob/main/examples/00_quick_start/lstur_MIND.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "featured-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "seed = 40\n",
    "batch_size = 32\n",
    "\n",
    "# Options: demo, small, large\n",
    "MIND_type = 'large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demographic-jonathan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LSTURModel(BaseModel):\n",
      "    \"\"\"LSTUR model(Neural News Recommendation with Multi-Head Self-Attention)\n",
      "\n",
      "    Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie:\n",
      "    Neural News Recommendation with Long- and Short-term User Representations, ACL 2019\n",
      "\n",
      "    Attributes:\n",
      "        word2vec_embedding (numpy.ndarray): Pretrained word embedding matrix.\n",
      "        hparam (object): Global hyper-parameters.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, hparams, iterator_creator, seed=None):\n",
      "        \"\"\"Initialization steps for LSTUR.\n",
      "        Compared with the BaseModel, LSTUR need word embedding.\n",
      "        After creating word embedding matrix, BaseModel's __init__ method will be called.\n",
      "\n",
      "        Args:\n",
      "            hparams (object): Global hyper-parameters. Some key setttings such as type and gru_unit are there.\n",
      "            iterator_creator_train (object): LSTUR data loader class for train data.\n",
      "            iterator_creator_test (object): LSTUR data loader class for test and validation data\n",
      "        \"\"\"\n",
      "\n",
      "        self.word2vec_embedding = self._init_embedding(hparams.wordEmb_file)\n",
      "        self.hparam = hparams\n",
      "\n",
      "        super().__init__(hparams, iterator_creator, seed=seed)\n",
      "\n",
      "    def _get_input_label_from_iter(self, batch_data):\n",
      "        input_feat = [\n",
      "            batch_data[\"user_index_batch\"],\n",
      "            batch_data[\"clicked_title_batch\"],\n",
      "            batch_data[\"candidate_title_batch\"],\n",
      "        ]\n",
      "        input_label = batch_data[\"labels\"]\n",
      "        return input_feat, input_label\n",
      "\n",
      "    def _get_user_feature_from_iter(self, batch_data):\n",
      "        return [batch_data[\"clicked_title_batch\"], batch_data[\"user_index_batch\"]]\n",
      "\n",
      "    def _get_news_feature_from_iter(self, batch_data):\n",
      "        return batch_data[\"candidate_title_batch\"]\n",
      "\n",
      "    def _build_graph(self):\n",
      "        \"\"\"Build LSTUR model and scorer.\n",
      "\n",
      "        Returns:\n",
      "            object: a model used to train.\n",
      "            object: a model used to evaluate and inference.\n",
      "        \"\"\"\n",
      "\n",
      "        model, scorer = self._build_lstur()\n",
      "        return model, scorer\n",
      "\n",
      "    def _build_userencoder(self, titleencoder, type=\"ini\"):\n",
      "        \"\"\"The main function to create user encoder of LSTUR.\n",
      "\n",
      "        Args:\n",
      "            titleencoder (object): the news encoder of LSTUR.\n",
      "\n",
      "        Return:\n",
      "            object: the user encoder of LSTUR.\n",
      "        \"\"\"\n",
      "        hparams = self.hparams\n",
      "        his_input_title = keras.Input(\n",
      "            shape=(hparams.his_size, hparams.title_size), dtype=\"int32\"\n",
      "        )\n",
      "        user_indexes = keras.Input(shape=(1,), dtype=\"int32\")\n",
      "\n",
      "        user_embedding_layer = layers.Embedding(\n",
      "            len(self.train_iterator.uid2index),\n",
      "            hparams.gru_unit,\n",
      "            trainable=True,\n",
      "            embeddings_initializer=\"zeros\",\n",
      "        )\n",
      "\n",
      "        long_u_emb = layers.Reshape((hparams.gru_unit,))(\n",
      "            user_embedding_layer(user_indexes)\n",
      "        )\n",
      "        click_title_presents = layers.TimeDistributed(titleencoder)(his_input_title)\n",
      "\n",
      "        if type == \"ini\":\n",
      "            user_present = layers.GRU(\n",
      "                hparams.gru_unit,\n",
      "                kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
      "                recurrent_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
      "                bias_initializer=keras.initializers.Zeros(),\n",
      "            )(\n",
      "                layers.Masking(mask_value=0.0)(click_title_presents),\n",
      "                initial_state=[long_u_emb],\n",
      "            )\n",
      "        elif type == \"con\":\n",
      "            short_uemb = layers.GRU(\n",
      "                hparams.gru_unit,\n",
      "                kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
      "                recurrent_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
      "                bias_initializer=keras.initializers.Zeros(),\n",
      "            )(layers.Masking(mask_value=0.0)(click_title_presents))\n",
      "\n",
      "            user_present = layers.Concatenate()([short_uemb, long_u_emb])\n",
      "            user_present = layers.Dense(\n",
      "                hparams.gru_unit,\n",
      "                bias_initializer=keras.initializers.Zeros(),\n",
      "                kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
      "            )(user_present)\n",
      "\n",
      "        model = keras.Model(\n",
      "            [his_input_title, user_indexes], user_present, name=\"user_encoder\"\n",
      "        )\n",
      "        return model\n",
      "\n",
      "    def _build_newsencoder(self, embedding_layer):\n",
      "        \"\"\"The main function to create news encoder of LSTUR.\n",
      "\n",
      "        Args:\n",
      "            embedding_layer (object): a word embedding layer.\n",
      "\n",
      "        Return:\n",
      "            object: the news encoder of LSTUR.\n",
      "        \"\"\"\n",
      "        hparams = self.hparams\n",
      "        sequences_input_title = keras.Input(shape=(hparams.title_size,), dtype=\"int32\")\n",
      "        embedded_sequences_title = embedding_layer(sequences_input_title)\n",
      "\n",
      "        y = layers.Dropout(hparams.dropout)(embedded_sequences_title)\n",
      "        y = layers.Conv1D(\n",
      "            hparams.filter_num,\n",
      "            hparams.window_size,\n",
      "            activation=hparams.cnn_activation,\n",
      "            padding=\"same\",\n",
      "            bias_initializer=keras.initializers.Zeros(),\n",
      "            kernel_initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
      "        )(y)\n",
      "        print(y)\n",
      "        y = layers.Dropout(hparams.dropout)(y)\n",
      "        y = layers.Masking()(\n",
      "            OverwriteMasking()([y, ComputeMasking()(sequences_input_title)])\n",
      "        )\n",
      "        pred_title = AttLayer2(hparams.attention_hidden_dim, seed=self.seed)(y)\n",
      "        print(pred_title)\n",
      "        model = keras.Model(sequences_input_title, pred_title, name=\"news_encoder\")\n",
      "        return model\n",
      "\n",
      "    def _build_lstur(self):\n",
      "        \"\"\"The main function to create LSTUR's logic. The core of LSTUR\n",
      "        is a user encoder and a news encoder.\n",
      "\n",
      "        Returns:\n",
      "            object: a model used to train.\n",
      "            object: a model used to evaluate and inference.\n",
      "        \"\"\"\n",
      "        hparams = self.hparams\n",
      "\n",
      "        his_input_title = keras.Input(\n",
      "            shape=(hparams.his_size, hparams.title_size), dtype=\"int32\"\n",
      "        )\n",
      "        pred_input_title = keras.Input(\n",
      "            shape=(hparams.npratio + 1, hparams.title_size), dtype=\"int32\"\n",
      "        )\n",
      "        pred_input_title_one = keras.Input(\n",
      "            shape=(\n",
      "                1,\n",
      "                hparams.title_size,\n",
      "            ),\n",
      "            dtype=\"int32\",\n",
      "        )\n",
      "        pred_title_reshape = layers.Reshape((hparams.title_size,))(pred_input_title_one)\n",
      "        user_indexes = keras.Input(shape=(1,), dtype=\"int32\")\n",
      "\n",
      "        embedding_layer = layers.Embedding(\n",
      "            self.word2vec_embedding.shape[0],\n",
      "            hparams.word_emb_dim,\n",
      "            weights=[self.word2vec_embedding],\n",
      "            trainable=True,\n",
      "        )\n",
      "\n",
      "        titleencoder = self._build_newsencoder(embedding_layer)\n",
      "        self.userencoder = self._build_userencoder(titleencoder, type=hparams.type)\n",
      "        self.newsencoder = titleencoder\n",
      "\n",
      "        user_present = self.userencoder([his_input_title, user_indexes])\n",
      "        news_present = layers.TimeDistributed(self.newsencoder)(pred_input_title)\n",
      "        news_present_one = self.newsencoder(pred_title_reshape)\n",
      "\n",
      "        preds = layers.Dot(axes=-1)([news_present, user_present])\n",
      "        preds = layers.Activation(activation=\"softmax\")(preds)\n",
      "\n",
      "        pred_one = layers.Dot(axes=-1)([news_present_one, user_present])\n",
      "        pred_one = layers.Activation(activation=\"sigmoid\")(pred_one)\n",
      "\n",
      "        model = keras.Model([user_indexes, his_input_title, pred_input_title], preds)\n",
      "        scorer = keras.Model(\n",
      "            [user_indexes, his_input_title, pred_input_title_one], pred_one\n",
      "        )\n",
      "\n",
      "        return model, scorer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "lines = inspect.getsource(LSTURModel)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-stupid",
   "metadata": {},
   "source": [
    "### Dowload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "personal-standing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd003/home/bjimenez/news_recommender'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gentle-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "data_path = cwd + '/data/' #tmpdir.name\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "valid_news_file = os.path.join(data_path, 'dev', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'dev', r'behaviors.tsv')\n",
    "\n",
    "data_path_LSTUR = cwd + '/data_LSTUR/' \n",
    "wordEmb_file = os.path.join(data_path_LSTUR, \"utils\", \"embedding.npy\")\n",
    "userDict_file = os.path.join(data_path_LSTUR, \"utils\", \"uid2index.pkl\")\n",
    "wordDict_file = os.path.join(data_path_LSTUR, \"utils\", \"word_dict.pkl\")\n",
    "yaml_file = os.path.join(data_path_LSTUR, \"utils\", r'lstur.yaml')\n",
    "\n",
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
    "    \n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url, \\\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
    "    \n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/', \\\n",
    "                               os.path.join(data_path_LSTUR, 'utils'), mind_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessible-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 4, 'head_dim': 100, 'filter_num': 400, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 5, 'batch_size': 32, 'show_step': 100000, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'cnn_activation': 'relu', 'model_type': 'lstur', 'loss': 'cross_entropy_loss', 'wordEmb_file': '/ssd003/home/bjimenez/news_recommender/data_LSTUR/utils/embedding.npy', 'wordDict_file': '/ssd003/home/bjimenez/news_recommender/data_LSTUR/utils/word_dict.pkl', 'userDict_file': '/ssd003/home/bjimenez/news_recommender/data_LSTUR/utils/uid2index.pkl'}\n"
     ]
    }
   ],
   "source": [
    "hparams = prepare_hparams(yaml_file, \n",
    "                          wordEmb_file=wordEmb_file,\n",
    "                          wordDict_file=wordDict_file, \n",
    "                          userDict_file=userDict_file,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "foreign-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = MINDIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-disco",
   "metadata": {},
   "source": [
    "### Train the LSTUR model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chronic-lambda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1d/Relu:0\", shape=(None, 30, 400), dtype=float32)\n",
      "Tensor(\"att_layer2/Sum_1:0\", shape=(None, 400), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd003/projects/aieng/public/recsys/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = LSTURModel(hparams, iterator, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "korean-queens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ssd003/home/bjimenez/news_recommender/data/dev/news.tsv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_news_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finite-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/ssd003/projects/aieng/public/recsys/lib/python3.7/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-11-28 10:43:48.494221: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-11-28 10:43:48.496181: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2251it [00:08, 270.35it/s]\n",
      "11765it [07:16, 26.98it/s]\n",
      "376471it [01:02, 5999.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_auc': 0.5248, 'mean_mrr': 0.2256, 'ndcg@5': 0.2341, 'ndcg@10': 0.2977}\n"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(valid_news_file, valid_behaviors_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2022-11-28 11:05:22.551121: W tensorflow/c/c_api.cc:349] Operation '{name:'user_encoder/gru/while' id:1585 op device:{requested: '', assigned: ''} def:{{{node user_encoder/gru/while}} = While[T=[DT_INT32, DT_INT32, DT_INT32, DT_VARIANT, DT_FLOAT, 3049133034371594848, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=113, _read_only_resource_inputs=[9, 10, 11], _stateful_parallelism=false, body=user_encoder_gru_while_body_2198[], cond=user_encoder_gru_while_cond_2197[], output_shapes=[[], [], [], [], [?,400], 5512068267592963614, [], [], [], [], []], parallel_iterations=32](user_encoder/gru/while/loop_counter, user_encoder/gru/while/maximum_iterations, user_encoder/gru/time, user_encoder/gru/TensorArrayV2_1, user_encoder/gru/zeros_like, user_encoder/reshape_1/Reshape, user_encoder/gru/strided_slice, user_encoder/gru/TensorArrayUnstack/TensorListFromTensor, user_encoder/gru/TensorArrayUnstack_1/TensorListFromTensor, gru/gru_cell/kernel, gru/gru_cell/bias, gru/gru_cell/recurrent_kernel, user_encoder/gru/while/EmptyTensorList, user_encoder/gru/while/EmptyTensorList_1, user_encoder/gru/while/EmptyTensorList_2, user_encoder/gru/while/EmptyTensorList_3, user_encoder/gru/while/EmptyTensorList_4, user_encoder/gru/while/EmptyTensorList_5, user_encoder/gru/while/EmptyTensorList_6, user_encoder/gru/while/EmptyTensorList_7, user_encoder/gru/while/EmptyTensorList_8, user_encoder/gru/while/EmptyTensorList_9, user_encoder/gru/while/EmptyTensorList_10, user_encoder/gru/while/EmptyTensorList_11, user_encoder/gru/while/EmptyTensorList_12, user_encoder/gru/while/EmptyTensorList_13, user_encoder/gru/while/EmptyTensorList_14, user_encoder/gru/while/EmptyTensorList_15, user_encoder/gru/while/EmptyTensorList_16, user_encoder/gru/while/EmptyTensorList_17, user_encoder/gru/while/EmptyTensorList_18, user_encoder/gru/while/EmptyTensorList_19, user_encoder/gru/while/EmptyTensorList_20, user_encoder/gru/while/EmptyTensorList_21, user_encoder/gru/while/EmptyTensorList_22, user_encoder/gru/while/EmptyTensorList_23, user_encoder/gru/while/EmptyTensorList_24, user_encoder/gru/while/EmptyTensorList_25, user_encoder/gru/while/EmptyTensorList_26, user_encoder/gru/while/EmptyTensorList_27, user_encoder/gru/while/EmptyTensorList_28, user_encoder/gru/while/EmptyTensorList_29, user_encoder/gru/while/EmptyTensorList_30, user_encoder/gru/while/EmptyTensorList_31, user_encoder/gru/while/EmptyTensorList_32, user_encoder/gru/while/EmptyTensorList_33, user_encoder/gru/while/EmptyTensorList_34, user_encoder/gru/while/EmptyTensorList_35, user_encoder/gru/while/EmptyTensorList_36, user_encoder/gru/while/EmptyTensorList_37, user_encoder/gru/while/EmptyTensorList_38, user_encoder/gru/while/EmptyTensorList_39, user_encoder/gru/while/EmptyTensorList_40, user_encoder/gru/while/EmptyTensorList_41, user_encoder/gru/while/EmptyTensorList_42, user_encoder/gru/while/EmptyTensorList_43, user_encoder/gru/while/EmptyTensorList_44, user_encoder/gru/while/EmptyTensorList_45, user_encoder/gru/while/EmptyTensorList_46, user_encoder/gru/while/EmptyTensorList_47, user_encoder/gru/while/EmptyTensorList_48, user_encoder/gru/while/EmptyTensorList_49, user_encoder/gru/while/EmptyTensorList_50, user_encoder/gru/while/EmptyTensorList_51, user_encoder/gru/while/EmptyTensorList_52, user_encoder/gru/while/EmptyTensorList_53, user_encoder/gru/while/EmptyTensorList_54, user_encoder/gru/while/EmptyTensorList_55, user_encoder/gru/while/EmptyTensorList_56, user_encoder/gru/while/EmptyTensorList_57, user_encoder/gru/while/EmptyTensorList_58, user_encoder/gru/while/EmptyTensorList_59, user_encoder/gru/while/EmptyTensorList_60, user_encoder/gru/while/EmptyTensorList_61, user_encoder/gru/while/EmptyTensorList_62, user_encoder/gru/while/EmptyTensorList_63, user_encoder/gru/while/EmptyTensorList_64, user_encoder/gru/while/EmptyTensorList_65, user_encoder/gru/while/EmptyTensorList_66, user_encoder/gru/while/EmptyTensorList_67, user_encoder/gru/while/EmptyTensorList_68, user_encoder/gru/while/EmptyTensorList_69, user_encoder/gru/while/EmptyTensorList_70, user_encoder/gru/while/EmptyTensorList_71, user_encoder/gru/while/EmptyTensorList_72, user_encoder/gru/while/EmptyTensorList_73, user_encoder/gru/while/EmptyTensorList_74, user_encoder/gru/while/EmptyTensorList_75, user_encoder/gru/while/EmptyTensorList_76, user_encoder/gru/while/EmptyTensorList_77, user_encoder/gru/while/EmptyTensorList_78, user_encoder/gru/while/EmptyTensorList_79, user_encoder/gru/while/EmptyTensorList_80, user_encoder/gru/while/EmptyTensorList_81, user_encoder/gru/while/EmptyTensorList_82, user_encoder/gru/while/EmptyTensorList_83, user_encoder/gru/while/EmptyTensorList_84, user_encoder/gru/while/EmptyTensorList_85, user_encoder/gru/while/EmptyTensorList_86, user_encoder/gru/while/EmptyTensorList_87, user_encoder/gru/while/EmptyTensorList_88, user_encoder/gru/while/EmptyTensorList_89, user_encoder/gru/while/EmptyTensorList_90, user_encoder/gru/while/EmptyTensorList_91, user_encoder/gru/while/EmptyTensorList_92, user_encoder/gru/while/EmptyTensorList_93, user_encoder/gru/while/EmptyTensorList_94, user_encoder/gru/while/EmptyTensorList_95, user_encoder/gru/while/EmptyTensorList_96, user_encoder/gru/while/EmptyTensorList_97, user_encoder/gru/while/EmptyTensorList_98, user_encoder/gru/while/EmptyTensorList_99, user_encoder/gru/while/EmptyTensorList_100)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "step 100000 , total_loss: 1.2658, data_loss: 1.0819: : 105740it [6:47:38,  4.32it/s]\n",
      "2251it [00:03, 679.73it/s]\n",
      "11765it [07:01, 27.90it/s]\n",
      "376471it [01:03, 5949.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1\n",
      "train info: logloss loss:1.2636026011447679\n",
      "eval info: group_auc:0.6819, mean_mrr:0.329, ndcg@10:0.4268, ndcg@5:0.3628\n",
      "at epoch 1 , train time: 24458.9 eval time: 846.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45628it [2:54:52,  4.30it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
    "print(res_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sb.glue(\"res_syn\", res_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-gazette",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(data_path, \"model\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "model.model.save_weights(os.path.join(model_path, \"lstur_ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-chorus",
   "metadata": {},
   "source": [
    "### Output prediction file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
    "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
    "        impr_index += 1\n",
    "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
    "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
    "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
    "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-wages",
   "metadata": {},
   "source": [
    "\n",
    "Reference\n",
    "\n",
    "[1] Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie: Neural News Recommendation with Long- and Short-term User Representations, ACL 2019\n",
    "\n",
    "[2] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html\n",
    "\n",
    "[3] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
